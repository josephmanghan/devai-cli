<story-context id="2-4-implement-custom-model-creation" v="1.0">
  <metadata>
    <epicId>2</epicId>
    <storyId>4</storyId>
    <title>Implement Custom Model Creation</title>
    <status>drafted</status>
    <generatedAt>2025-11-30</generatedAt>
    <generator>BMAD Story Context Workflow</generator>
    <sourceStoryPath>dev/sprint-artifacts/2-4-implement-custom-model-creation.md</sourceStoryPath>
  </metadata>

  <story>
    <asA>a developer</asA>
    <iWant>to create the custom `ollatool-commit` model from a Modelfile</iWant>
    <soThat>the system prompt is baked into the model instance for consistent commit message generation</soThat>
    <tasks>- Create Modelfile with system prompt and parameters (AC: 1, 6, 7)
  - [ ] Create `Modelfile` in project root with FROM qwen2.5-coder:1.5b
  - [ ] Define SYSTEM prompt with Conventional Commits expert role
  - [ ] Include few-shot examples for format consistency
  - [ ] Set PARAMETER values: temperature 0.2, num_ctx 131072
- Implement createModel() method in OllamaAdapter (AC: 2, 3)
  - [ ] Add createModel() method to OllamaAdapter class
  - [ ] Use ollama client.create() with modelfile content
  - [ ] Set model name to 'ollatool-commit:latest'
  - [ ] Handle model creation failures with ValidationError
- Add idempotency and progress feedback (AC: 4, 5)
  - [ ] Check if model exists before creation using checkModel()
  - [ ] Display progress spinner during model creation
  - [ ] Skip creation if model already exists with informational message
  - [ ] Include clear success/failure messaging
- Add comprehensive error handling (AC: 2)
  - [ ] Handle modelfile read errors with clear guidance
  - [ ] Wrap ollama SDK errors in ValidationError with context
  - [ ] Provide actionable remediation for creation failures
  - [ ] Include debug information for troubleshooting
- Create unit tests for createModel functionality
  - [ ] Test successful model creation with mock SDK
  - [ ] Test idempotency when model already exists
  - [ ] Test error handling for modelfile issues
  - [ ] Test error handling for SDK creation failures
- Add integration test for manual validation
  - [ ] Create test script to validate actual model creation
  - [ ] Include cleanup step to remove test model
  - [ ] Document manual testing procedure for developers</tasks>
  </story>

  <acceptanceCriteria>1. [ ] Modelfile created defining system prompt and base model
2. [ ] OllamaAdapter.createModel() implemented using ollama SDK `create()` method
3. [ ] Model name set to `ollatool-commit:latest` per architecture specification
4. [ ] Custom model creation is idempotent (safe to run multiple times)
5. [ ] Progress feedback during model creation with optional spinner
6. [ ] System prompt defines Conventional Commits expert role with few-shot examples
7. [ ] Model parameters configured: temperature=0.2, num_ctx=131072, keep_alive=0</acceptanceCriteria>

  <artifacts>
    <docs>
      <doc>
        <path>dev/architecture.md</path>
        <title>Ollama Model Architecture</title>
        <section>Ollama Model Architecture</section>
        <snippet>Architecture specifies Modelfile-based prompt engineering where system prompt is baked into model instance. Base Model: Uses `qwen2.5-coder:1.5b` as foundation per architecture decision for sub-1s performance on M1/M2 hardware.</snippet>
      </doc>
      <doc>
        <path>dev/architecture.md</path>
        <title>Prompt Engineering Architecture</title>
        <section>Prompt Engineering Architecture</section>
        <snippet>System Prompt (Modelfile - Static): Role definition and behavioral constraints, Conventional Commits format rules, Few-shot examples demonstrating ideal outputs, Output format constraints (no conversational filler)</snippet>
      </doc>
      <doc>
        <path>dev/architecture.md</path>
        <title>Model Instance Design</title>
        <section>Ollama Model Architecture</section>
        <snippet>Custom Model Instance: `ollatool-commit` Base Model: `qwen2.5-coder:1.5b` (quantized) Creation Method: Modelfile-based instance creation</snippet>
      </doc>
      <doc>
        <path>dev/architecture.md</path>
        <title>Ollama Integration Parameters</title>
        <section>Ollama Integration Parameters</section>
        <snippet>temperature: 0.2 for low randomness and deterministic, consistent outputs, num_ctx: 131072 Full context window capacity (128K tokens), keep_alive: 0 MVP: Unload model after each execution. Clean lifecycle: load → infer → commit → unload</snippet>
      </doc>
      <doc>
        <path>dev/architecture.md</path>
        <title>Context7 MCP Integration</title>
        <section>Context7 MCP Integration</section>
        <snippet>Context7 MCP server is available and MUST be used for enterprise-grade code quality validation, particularly starting with Epic 2 implementation. Use `mcp__context7__resolve-library-id` to locate authoritative documentation, retrieve latest best practices via `mcp__context7__get-library-docs`</snippet>
      </doc>
      <doc>
        <path>dev/stories/epic-2-ollama-integration.md</path>
        <title>Epic 2 Story 2.4</title>
        <section>Story 2.4: Implement Custom Model Creation</section>
        <snippet>Technical Notes: Architecture specifies Modelfile-based prompt engineering. Base: `FROM qwen2.5-coder:1.5b`. System prompt defines Conventional Commits expert role. Parameters: temperature=0.2, num_ctx=131072, keep_alive=0. Modelfile content comes from prompt engineering research</snippet>
      </doc>
    </docs>
    <code>
      <artifact>
        <path>src/core/ports/llm-port.ts</path>
        <kind>interface</kind>
        <symbol>LlmPort.createModel</symbol>
        <lines>24-33</lines>
        <reason>Interface definition for createModel method that this story will implement. Already defines method signature with proper error handling requirements.</reason>
      </artifact>
      <artifact>
        <path>src/infrastructure/llm/ollama-adapter.ts</path>
        <kind>class</kind>
        <symbol>OllamaAdapter.createModel</symbol>
        <lines>57-69</lines>
        <reason>Current createModel implementation - currently stub implementation that needs to be enhanced to support Modelfile content and idempotency checking.</reason>
      </artifact>
      <artifact>
        <path>src/infrastructure/llm/ollama-adapter.ts</path>
        <kind>class</kind>
        <symbol>OllamaAdapter.checkModel</symbol>
        <lines>42-49</lines>
        <reason>Existing checkModel method used for idempotency - can check if model exists before attempting creation.</reason>
      </artifact>
      <artifact>
        <path>src/infrastructure/llm/ollama-adapter.ts</path>
        <kind>class</kind>
        <symbol>OllamaAdapter.wrapOllamaError</symbol>
        <lines>99-110</lines>
        <reason>Existing error handling patterns that should be used for createModel error wrapping.</reason>
      </artifact>
      <artifact>
        <path>src/core/types/llm-types.ts</path>
        <kind>interface</kind>
        <symbol>GenerationOptions</symbol>
        <lines>10-33</lines>
        <reason>Type definitions for generation parameters - relevant for understanding model parameter configuration (temperature, num_ctx, keep_alive).</reason>
      </artifact>
    </code>
    <dependencies>
      <ecosystem name="nodejs">
        <package name="ollama" version="0.6.3" purpose="Official Ollama SDK for model operations (create, list, generate)" />
        <package name="commander" version="14.0.2" purpose="CLI framework for setup command integration" />
        <package name="vitest" version="4.0.14" purpose="Test framework for unit and integration testing" />
        <package name="typescript" version="5.9.3" purpose="Type safety and strict mode compilation" />
        <package name="ora" version="8.2.0" purpose="Terminal spinner for model creation progress feedback" />
      </ecosystem>
    </dependencies>
    <context7Patterns>
      <ollamaCreateMethod validated="true" source="/ollama/ollama-js">
        <pattern name="model-creation" implementation="ollama.create({ model, modelfile, stream: true })">
          <description>Use ollama.create() with stream parameter for progress feedback during model creation</description>
          <validation>Context7 MCP validated pattern from official Ollama JS documentation</validation>
        </pattern>
      </ollamaCreateMethod>
      <progressSpinner validated="true" source="ora-integration">
        <pattern name="streaming-progress" implementation="ora spinner + ollama.create stream">
          <description>Integrate ora spinner with ollama.create() stream parameter for real-time progress updates</description>
          <validation>Combines established ora patterns with Ollama SDK streaming</validation>
        </pattern>
      </progressSpinner>
    </context7Patterns>
    <constraints>
      <constraint source="dev/architecture.md">Clean Code Standards: All methods must be ≤15 lines, use constructor dependency injection, follow hexagonal architecture patterns, import grouping: external → core → feature-specific</constraint>
      <constraint source="dev/architecture.md">Context7 MCP Integration: MUST use Context7 MCP server for Ollama SDK validation - `mcp__context7__resolve-library-id("ollama")` and `mcp__context7__get-library-docs` for best practices</constraint>
      <constraint source="dev/architecture.md">Error Handling: Use typed error classes (ValidationError, SystemError) with actionable remediation, catch at boundaries not everywhere</constraint>
      <constraint source="dev/architecture.md">File Organization: One class per file, co-located tests (.test.ts), kebab-case file naming, export at bottom of file</constraint>
      <constraint source="dev/architecture.md">Architecture: Modelfile in project root, createModel() in OllamaAdapter, use existing checkModel() for idempotency</constraint>
      <constraint source="dev/architecture.md">Model Configuration: Temperature=0.2, num_ctx=131072, keep_alive=0 as per architecture decision table</constraint>
    </constraints>
    <interfaces>
      <interface name="LlmPort.createModel" kind="method signature">
        <signature>createModel(modelName: string, modelDefinition: string): Promise<void></signature>
        <path>src/core/ports/llm-port.ts:24-33</path>
        <purpose>Port method that this story implements in OllamaAdapter</purpose>
      </interface>
      <interface name="Ollama SDK Create" kind="sdk method">
        <signature>client.create({ model: string, modelfile?: string, stream?: boolean })</signature>
        <purpose>Context7 validated Ollama SDK method for model creation from Modelfile with streaming support</purpose>
      </interface>
    </interfaces>
    <modelfileStructure>
      <template validated="true" source="dev/architecture.md">
        <structure>
          <line>FROM qwen2.5-coder:1.5b</line>
          <line>SYSTEM """[Conventional Commits expert role with few-shot examples]"""</line>
          <line>PARAMETER temperature 0.2</line>
          <line>PARAMETER num_ctx 131072</line>
        </structure>
        <validation>Modelfile syntax follows Ollama specification with proper SYSTEM prompt and PARAMETER formatting</validation>
      </template>
    </modelfileStructure>
    <tests>
      <standards>Use Vitest with vi.fn() for mocking Ollama SDK, test all error paths (ValidationError, SystemError), follow co-located test pattern, test both successful creation and idempotency when model exists, validate Context7 MCP integration patterns</standards>
      <locations>Co-located test: src/infrastructure/llm/ollama-adapter.test.ts, use existing factory patterns for test data, follow established mock patterns from existing tests</locations>
      <ideas>
        <test ac="1,2,3">Test createModel calls ollama.create() with correct modelName and modelfile content</test>
        <test ac="4">Test idempotency: when checkModel() returns true, createModel should skip creation</test>
        <test ac="5">Test error handling: file read errors wrapped in ValidationError, SDK errors handled with existing wrapOllamaError pattern</test>
        <test ac="6,7">Test model name 'ollatool-commit:latest' and verify parameters in Modelfile (temperature=0.2, num_ctx=131072)</test>
        <test ac="all">Integration test for manual validation of actual model creation on dev machine</test>
      </ideas>
    </tests>
  </artifacts>
</story-context>