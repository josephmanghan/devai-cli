# ðŸ“ Project Log: Local AI Command Line Wrapper

**Date:** November 25, 2025
**Session Goal:** Install Ollama, verify the pipeline, and define the architecture for a custom CLI tool.

## 1. Project Vision

- **Objective:** Build a command-line wrapper application around Ollama.
- **Function:** The app will utilize local SLMs (Small Language Models) to perform specific tasks via code, rather than just chat.
- **Target Audience:** Needs to run on "standard" hardware (e.g., 8GB/16GB laptops), not just high-end dev machines.
- **Tech Stack:** Node.js (utilizing the `ollama` npm library) talking to the local Ollama server.

## 2. Hardware Context

- **Dev Machine:** MacBook Pro (M3 Pro Chip, 36GB Unified Memory).
- **Capacity:** Capable of running large quantization models (up to ~24GB VRAM usage), but development will focus on smaller models to ensure end-user compatibility.

## 3. What We Accomplished

- **Installation:** Ollama is installed and running as a background service (Menu bar icon active).
- **Connectivity:** Verified communication between ZSH terminal and the Ollama API.
- **Network Fix:** Identified and bypassed a local Squid proxy issue using `curl --noproxy "*"`.
- **Model Management:** established a structured workflow for custom models (`~/ai-models/`) rather than creating files in the root.
- **Architecture Validation:** Confirmed that the app should use the **Client-Server model** (App -> Ollama Library -> Ollama Server -> Hardware).

## 4. The "Cheat Sheet" (Key Commands Learned)

| Action            | Command                          | Note                                              |
| :---------------- | :------------------------------- | :------------------------------------------------ |
| **Start Chat**    | `ollama run <model>`             | Interactive mode.                                 |
| **Exit Chat**     | `/bye` or `Ctrl + d`             | Exits window, but keeps model loaded for ~5 mins. |
| **Check Usage**   | `ollama ps`                      | Shows CPU/GPU memory usage of _active_ models.    |
| **Force Stop**    | `ollama stop <model>`            | Instantly unloads model from RAM.                 |
| **List Models**   | `ollama list`                    | Shows all installed models.                       |
| **Create Custom** | `ollama create <name> -f <path>` | "Bakes the cake" from the Modelfile recipe.       |
| **API Test**      | `curl --noproxy "*" ...`         | Required to bypass local proxy settings.          |

## 5. Model Strategy

We selected specific models for different purposes to balance performance vs. compatibility:

- **For Pipeline Testing:** `smollm:135m`
  - _Why:_ Tiny (135MB), instant download, proves the code works.
- **For Your Development (Coding Helper):** `qwen2.5-coder:14b`
  - _Why:_ Optimized for code generation, fits easily in your 36GB RAM.
- **For The Application (The "Product" Model):** `llama3.2` (3B)
  - _Why:_ The "Goldilocks" choice. Smart enough to follow instructions, but small enough (~2GB) to run on an average user's 8GB laptop.

## 6. Next Steps (For Future Session)

1.  Initialize the Node.js project.
2.  Install the library: `npm install ollama`.
3.  Write the first script to programmatically send a prompt to `llama3.2` and parse the response object (replacing the manual `curl` method).

---

**Status:** Ready to code. The environment is verified and the proxy issues are resolved.
